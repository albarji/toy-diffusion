{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9887c243",
   "metadata": {},
   "source": [
    "# Toy diffusion model generating data from a complex 1-d distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b284f76",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0311948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "generator = np.random.default_rng(seed=12345)\n",
    "# Uniform numbers over the range [0, 1]\n",
    "#x = generator.uniform(low=0.0, high=1.0, size=100000)\n",
    "x = generator.uniform(low=0.0, high=1.0, size=10000)\n",
    "# Add gaussian centered in 0.2\n",
    "x = np.concatenate([x, generator.normal(0.2, 1, size=10000)])\n",
    "# Add another high variance gaussian centered in 5\n",
    "x = np.concatenate([x, generator.normal(5, 5, size=20000)])\n",
    "# Add another low variance gaussian centered in 15\n",
    "x = np.concatenate([x, generator.normal(15, 1, size=10000)])\n",
    "# Normalize and center\n",
    "x = (x - x.mean()) / x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3a0237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(x, kde=True, bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7472c8",
   "metadata": {},
   "source": [
    "## Prepare data for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de92ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "X = rearrange(torch.tensor(x, dtype=torch.float32), \"x -> x 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9166d03",
   "metadata": {},
   "source": [
    "## Noising functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e934300",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_steps = 40\n",
    "\n",
    "s = 0.008\n",
    "timesteps = torch.tensor(range(0, diffusion_steps), dtype=torch.float32)\n",
    "schedule = torch.cos((timesteps / diffusion_steps + s) / (1 + s) * torch.pi / 2)**2\n",
    "\n",
    "baralphas = schedule / schedule[0]\n",
    "betas = 1 - baralphas / torch.concatenate([baralphas[0:1], baralphas[0:-1]])\n",
    "alphas = 1 - betas\n",
    "sigmas = torch.sqrt(betas)\n",
    "\n",
    "sns.lineplot(baralphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55680085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(Xbatch, t):\n",
    "    if torch.is_tensor(t):\n",
    "        t = t.flatten()\n",
    "    eps = torch.randn(size=(len(Xbatch), 1))\n",
    "    noised = rearrange(torch.sqrt(baralphas[t]), \"x -> x 1\") * Xbatch + rearrange(torch.sqrt(1 - baralphas[t]), \"x -> x 1\") * eps\n",
    "    return noised, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816df6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "noiselevel = 20\n",
    "\n",
    "noised, eps = noise(X, [noiselevel] * len(X))\n",
    "sns.displot(X, kde=True, bins=100)\n",
    "sns.displot(noised, kde=True, bins=100)\n",
    "denoised = 1 / torch.sqrt(baralphas[noiselevel]) * (noised - torch.sqrt(1 - baralphas[noiselevel]) * eps)\n",
    "sns.displot(denoised, kde=True, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "X - denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39202a7c",
   "metadata": {},
   "source": [
    "## Diffusion network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8518c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class PolynomialExpansionLayer(nn.Module):\n",
    "    \"\"\"Custom layer that expands the given variables into nonlinear combinations following a polynomial of d degree\"\"\"\n",
    "    def __init__(self, degree):\n",
    "        super().__init__()\n",
    "        self.degree = degree\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = x.shape[-1]\n",
    "        result = x.clone()\n",
    "        for exp in range(2, self.degree+1):\n",
    "            result = torch.hstack([result, x ** exp])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88caffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "print(x)\n",
    "\n",
    "PolynomialExpansionLayer(3)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266dd9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Custom layer that adds features in a fashion similar to a transformer positional encoding\"\"\"\n",
    "\n",
    "    def __init__(self, t_index: int, max_t: int, d: int = 256):\n",
    "        super().__init__()\n",
    "        self.t_index = t_index\n",
    "\n",
    "        position = torch.arange(max_t).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d, 2) * (-math.log(10000.0) / d))\n",
    "        pe = torch.zeros(max_t, d)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        return torch.hstack([x, self.pe[x[:, self.t_index].long()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02139c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1, 2], [3, 4], [200, 6]])\n",
    "print(x)\n",
    "\n",
    "PositionalEncoding(1, 1000, 32)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef2bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(2, 256),\n",
    "    nn.LayerNorm(256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 512),\n",
    "    nn.LayerNorm(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(512, 1024),\n",
    "    nn.LayerNorm(1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 1)\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c9b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old implementation\n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(2, 1024)\n",
    "        self.norm1 = nn.LayerNorm(1024)\n",
    "        self.linear2 = nn.Linear(1025, 1024)\n",
    "        self.norm2 = nn.LayerNorm(1024)\n",
    "        self.linear3 = nn.Linear(1025, 1024)\n",
    "        self.norm3 = nn.LayerNorm(1024)\n",
    "        self.linear4 = nn.Linear(1025, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        val = torch.hstack([x, t])  # Add t to inputs\n",
    "        val = self.linear1(val)\n",
    "        val = self.norm1(val)\n",
    "        val = nn.functional.relu(val)\n",
    "        val = torch.hstack([val, t])  # Add t again\n",
    "        val = self.linear2(val)\n",
    "        val = self.norm2(val)\n",
    "        val = nn.functional.relu(val)\n",
    "        val = torch.hstack([val, t])  # Add t again\n",
    "        val = self.linear3(val)\n",
    "        val = self.norm3(val)\n",
    "        val = nn.functional.relu(val)\n",
    "        val = torch.hstack([val, t])  # Add t again\n",
    "        val = self.linear4(val)\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94885f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DiffusionBlock(nn.Module):\n",
    "    def __init__(self, nunits):\n",
    "        super(DiffusionBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(nunits+1, nunits+1)\n",
    "        self.norm1 = nn.LayerNorm(nunits+1)\n",
    "        self.linear2 = nn.Linear(nunits+1, nunits)\n",
    "        self.norm2 = nn.LayerNorm(nunits)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        val = torch.hstack([x, t])  # Add t to inputs\n",
    "        val = self.linear1(val)\n",
    "        val = self.norm1(val)\n",
    "        val = nn.functional.relu(val)\n",
    "        val = self.linear2(val)\n",
    "        val = self.norm2(val)\n",
    "        val = nn.functional.relu(val)\n",
    "        return val + x  # Skip connection\n",
    "        \n",
    "    \n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, nblocks: int = 2, nunits: int = 64):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        \n",
    "        self.inblock = nn.Linear(2, nunits)\n",
    "        self.midblocks = nn.ModuleList([DiffusionBlock(nunits) for _ in range(nblocks)])\n",
    "        self.outblock = nn.Linear(nunits, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        val = torch.hstack([x, t])  # Add t to inputs\n",
    "        val = self.inblock(val)\n",
    "        for midblock in self.midblocks:\n",
    "            val = midblock(val, t)\n",
    "        val = self.outblock(val)\n",
    "        return val\n",
    "\n",
    "model = DiffusionModel(nblocks=5)\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe17c88",
   "metadata": {},
   "source": [
    "Denoising model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d045d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 2048\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01, total_iters=nepochs)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    epoch_loss = steps = 0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        timesteps = torch.randint(0, diffusion_steps, size=[len(Xbatch), 1])\n",
    "        noised, eps = noise(Xbatch, timesteps)\n",
    "        #predicted_noise = model(torch.hstack([noised, timesteps]).to(device))\n",
    "        predicted_noise = model(noised.to(device), timesteps.to(device))\n",
    "        loss = loss_fn(predicted_noise, eps.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "        steps += 1\n",
    "    print(f\"Epoch {epoch} loss = {epoch_loss / steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844512d",
   "metadata": {},
   "source": [
    "Best model (complex problem):\n",
    "* Without t: 0.003909660503268242\n",
    "* With t as a second input: Epoch 99 loss = 0.0016174211632460356\n",
    "* Custom network with 4 linear units (256 -> 512 -> 1024 -> 1) with ReLU, LayerNorm, and t reinjection, batchsize=1024: Epoch 99 loss = 0.0004879182088188827\n",
    "\n",
    "After fixing issues with loss reporting:\n",
    "* Custom network with 4 linear units (256 -> 512 -> 1024 -> 1) with ReLU, LayerNorm, and t reinjection, batchsize=1024: Epoch 99 loss = 0.5827606320381165\n",
    "* Custom network with 4 linear units (1024 -> 1024 -> 1024 -> 1) with ReLU, LayerNorm, and t reinjection, batchsize=2048: Epoch 999 loss = 0.530308723449707"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6c44f",
   "metadata": {},
   "source": [
    "## Test sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, nsamples):\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(size=(nsamples, 1)).to(device)\n",
    "        for t in range(diffusion_steps-1, 0, -1):\n",
    "            # predicted_noise = model(torch.hstack([x, torch.ones(size=[nsamples, 1]).to(device) * t]))\n",
    "            predicted_noise = model(x, torch.ones(size=[nsamples, 1]).to(device) * t)\n",
    "            # Predict x0 using DDPM equations\n",
    "            # x = 1 / torch.sqrt(alphas[t]) * (x - betas[t] / torch.sqrt(1 - baralphas[t]) * predicted_noise)\n",
    "            # x = x - (1 - alphas[t]) / torch.sqrt(1 - baralphas[t]) * predicted_noise\n",
    "            # Predict original sample using DDPM Eq. 15\n",
    "            x0 = (x - (1 - baralphas[t]) ** (0.5) * predicted_noise) / baralphas[t] ** (0.5)\n",
    "            # Predict previous samples using DDPM Eq. 7\n",
    "            c0 = (baralphas[t-1] ** (0.5) * betas[t]) / (1 - baralphas[t])\n",
    "            ct = alphas[t] ** (0.5) * (1 - baralphas[t-1]) / (1 - baralphas[t])\n",
    "            x = c0 * x0 + ct * x\n",
    "            if t > 1:\n",
    "                variance = (1 - baralphas[t-1]) / (1 - baralphas[t]) * betas[t]\n",
    "                variance = torch.clamp(variance, min=1e-20)\n",
    "                std = variance ** (0.5)\n",
    "                x += std * torch.randn(size=(nsamples, 1)).to(device)\n",
    "                # x += sigmas[t] * torch.randn(size=(nsamples, 1))\n",
    "                # x += (1 - baralphas[t-1]) / (1 - baralphas[t]) * betas[t] * torch.randn(size=(nsamples, 1))\n",
    "            # print(f\"Step {t} = {x[0]}\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aed25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgen = sample(model, 10000).cpu()\n",
    "sns.displot(X, kind=\"kde\")\n",
    "sns.displot(Xgen, kind=\"kde\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2676464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e9482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
