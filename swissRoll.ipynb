{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9887c243",
   "metadata": {},
   "source": [
    "# Toy diffusion model generating data from a complex 2-d distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b88bba",
   "metadata": {},
   "source": [
    "General imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf06f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b284f76",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0311948",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "\n",
    "x, _ = make_swiss_roll(n_samples=100000, noise=0.5)\n",
    "# Make two-dimensional to easen visualization\n",
    "x = x[:, [0, 2]]\n",
    "\n",
    "x = (x - x.mean()) / x.std()\n",
    "\n",
    "plt.scatter(x[:, 0], x[:, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7472c8",
   "metadata": {},
   "source": [
    "## Prepare data for learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de92ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(x, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9166d03",
   "metadata": {},
   "source": [
    "## Noising functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e934300",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_steps = 40\n",
    "\n",
    "s = 0.008\n",
    "timesteps = torch.tensor(range(0, diffusion_steps), dtype=torch.float32)\n",
    "schedule = torch.cos((timesteps / diffusion_steps + s) / (1 + s) * torch.pi / 2)**2\n",
    "\n",
    "baralphas = schedule / schedule[0]\n",
    "betas = 1 - baralphas / torch.concatenate([baralphas[0:1], baralphas[0:-1]])\n",
    "alphas = 1 - betas\n",
    "\n",
    "sns.lineplot(baralphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55680085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "\n",
    "def noise(Xbatch, t):\n",
    "    # t = rearrange(t, \"x -> x 1\")  # t to column tensor\n",
    "    eps = torch.randn(size=Xbatch.shape)\n",
    "    noised = (baralphas[t] ** 0.5).repeat(1, Xbatch.shape[1]) * Xbatch + ((1 - baralphas[t]) ** 0.5).repeat(1, Xbatch.shape[1]) * eps\n",
    "    return noised, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816df6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "noiselevel = 20\n",
    "\n",
    "noised, eps = noise(X, torch.full([len(X), 1], fill_value=noiselevel))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(noised[:, 0], noised[:, 1], marker=\"*\")\n",
    "denoised = 1 / torch.sqrt(baralphas[noiselevel]) * (noised - torch.sqrt(1 - baralphas[noiselevel]) * eps)\n",
    "plt.scatter(denoised[:, 0], denoised[:, 1], marker=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c10c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "X - denoised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39202a7c",
   "metadata": {},
   "source": [
    "## Diffusion network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94885f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DiffusionBlock(nn.Module):\n",
    "    def __init__(self, nunits):\n",
    "        super(DiffusionBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(nunits+1, nunits+1)\n",
    "        self.norm1 = nn.LayerNorm(nunits+1)\n",
    "        self.linear2 = nn.Linear(nunits+1, nunits)\n",
    "        self.norm2 = nn.LayerNorm(nunits)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
    "        val = torch.hstack([x, t])  # Add t to inputs\n",
    "        val = self.linear1(val)\n",
    "        val = self.norm1(val)\n",
    "        val = nn.functional.relu(val)\n",
    "        val = self.linear2(val)\n",
    "        val = self.norm2(val)\n",
    "        val = nn.functional.relu(val)\n",
    "        return val + x  # Skip connection\n",
    "        \n",
    "    \n",
    "class DiffusionModel(nn.Module):\n",
    "    def __init__(self, nfeatures: int, nblocks: int = 2, nunits: int = 64):\n",
    "        super(DiffusionModel, self).__init__()\n",
    "        \n",
    "        self.inblock = nn.Linear(nfeatures+1, nunits)\n",
    "        self.midblocks = nn.ModuleList([DiffusionBlock(nunits) for _ in range(nblocks)])\n",
    "        self.outblock = nn.Linear(nunits, nfeatures)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        val = torch.hstack([x, t])  # Add t to inputs\n",
    "        val = self.inblock(val)\n",
    "        for midblock in self.midblocks:\n",
    "            val = midblock(val, t)\n",
    "        val = self.outblock(val)\n",
    "        return val\n",
    "\n",
    "model = DiffusionModel(nfeatures=2, nblocks=2)\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe17c88",
   "metadata": {},
   "source": [
    "Denoising model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d045d15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from einops import rearrange\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "nepochs = 100\n",
    "batch_size = 2048\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.01, total_iters=nepochs)\n",
    "\n",
    "for epoch in range(nepochs):\n",
    "    epoch_loss = steps = 0\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        timesteps = torch.randint(0, diffusion_steps, size=[len(Xbatch), 1])\n",
    "        noised, eps = noise(Xbatch, timesteps)\n",
    "        predicted_noise = model(noised.to(device), timesteps.to(device))\n",
    "        loss = loss_fn(predicted_noise, eps.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss\n",
    "        steps += 1\n",
    "    print(f\"Epoch {epoch} loss = {epoch_loss / steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844512d",
   "metadata": {},
   "source": [
    "Best model: \n",
    "* DiffusionModel(nfeatures=2, nblocks=5), 40 diffusion steps, batchsize=2048, Epoch 99 loss = 0.4353588819503784\n",
    "* DiffusionModel(nfeatures=2, nblocks=2), 40 diffusion steps, batchsize=2048, Epoch 99 loss = 0.4339998960494995\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f6c44f",
   "metadata": {},
   "source": [
    "## Test sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f970fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ddpm(model, nsamples, nfeatures):\n",
    "    \"\"\"Sampler following the Denoising Diffusion Probabilistic Models method by Ho et al (Algorithm 2)\"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(size=(nsamples, nfeatures)).to(device)\n",
    "        for t in range(diffusion_steps-1, 0, -1):\n",
    "            predicted_noise = model(x, torch.full([nsamples, 1], t).to(device))\n",
    "            # See DDPM paper between equations 11 and 12\n",
    "            x = 1 / (alphas[t] ** 0.5) * (x - (1 - alphas[t]) / ((1-baralphas[t]) ** 0.5) * predicted_noise)\n",
    "            if t > 1:\n",
    "                # See DDPM paper section 3.2.\n",
    "                # Choosing the variance through beta_t is optimal for x_0 a normal distribution (what we use here)\n",
    "                variance = betas[t]\n",
    "                std = variance ** (0.5)\n",
    "                x += std * torch.randn(size=(nsamples, nfeatures)).to(device)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe9cb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgen = sample_ddpm(model, 10000, 2).cpu()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(Xgen[:, 0], Xgen[:, 1], marker=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2de618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ddpm_x0(model, nsamples, nfeatures):\n",
    "    \"\"\"Sampler that uses the equations in DDPM paper to predict x0, then use that to predict x_{t-1}\n",
    "    \n",
    "    This is how DDPM is implemented in HuggingFace Diffusers, to allow working with models that predict\n",
    "    x0 instead of the noise. It is also how we explain it in the Mixture of Diffusers paper.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        x = torch.randn(size=(nsamples, nfeatures)).to(device)\n",
    "        for t in range(diffusion_steps-1, 0, -1):\n",
    "            predicted_noise = model(x, torch.full([nsamples, 1], t).to(device))\n",
    "            # Predict original sample using DDPM Eq. 15\n",
    "            x0 = (x - (1 - baralphas[t]) ** (0.5) * predicted_noise) / baralphas[t] ** (0.5)  # CHECKED!\n",
    "            # Predict previous sample using DDPM Eq. 7\n",
    "            c0 = (baralphas[t-1] ** (0.5) * betas[t]) / (1 - baralphas[t])\n",
    "            ct = alphas[t] ** (0.5) * (1 - baralphas[t-1]) / (1 - baralphas[t])\n",
    "            x = c0 * x0 + ct * x\n",
    "            # Add noise\n",
    "            if t > 1:\n",
    "                # variance = (1 - baralphas[t-1]) / (1 - baralphas[t]) * betas[t] # CHECKED!\n",
    "                variance = betas[t]\n",
    "                variance = torch.clamp(variance, min=1e-20) # CHECKED!\n",
    "                std = variance ** (0.5) # CHECKED!\n",
    "                x += std * torch.randn(size=(nsamples, nfeatures)).to(device) # CHECKED!\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0418697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgen = sample_ddpm_x0(model, 10000, 2).cpu()\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.scatter(Xgen[:, 0], Xgen[:, 1], marker=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063e9482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
